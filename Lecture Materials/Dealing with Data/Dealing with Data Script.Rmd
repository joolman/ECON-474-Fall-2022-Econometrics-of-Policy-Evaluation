---
title: "Dealing with Data"
subtitle: "ECON 474 Econometrics of Policy Evaluation"
output: 
  html_document:
    toc: yes
    toc_float: true
    number_sections: true
    theme: cerulean
    highlight: pygments
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Overview**: 

To demonstrate the impressive capabilities of the collection of packages within the `tidyverse` packages, we are going to use the microdata (individual-level) from the current population survey (CPS) to replicated the [official unemployment numbers](https://fred.stlouisfed.org/series/UNRATE) for 2020.
The microdata are sourced from [IPUMS](https://cps.ipums.org/cps/index.shtml).
We are also going to look at unemployment rates of this time period for a few subpopulations too.

Then, we are going to bring in daily county-level confirmed covid cases from the [Github COVID-19 Data Repository](https://github.com/CSSEGISandData/COVID-19) by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University to determine if there are any correlations between monthly state-level unemployment rates and per-capita COVID cases.


# Data Cleaning and Manipulation 
********************************************************************************

> “Happy families are all alike; every unhappy family is unhappy in its own way.” –– Leo Tolstoy

> “Tidy datasets are all alike, but every messy dataset is messy in its own way.” –– Hadley Wickham

Most classes which analyze data provide data sets that are ready-to-go for analysis. 
The reality is that the real-world data you will download are almost never formatted to run your analysis.
These data can be plagued with

- ill-formatted variables
- categorical variables coded with numerical values
- `NA` codes that are not blank such as `.` or `-999`

It is up to us to prepare our "wild" data.

## Properly Formatted ("tidy") Data
************************************************************

Each data set should abide by the following three rules as per [R4DS](https://r4ds.had.co.nz/tidy-data.html)

1. Each variable must have its own column
2. Each observation must have its own row
3. Each value must have its own cell

![](tidy-1.png)


Fortunately, Dr. Hadley Wickham (the Chief Scientist at RStudio) and his many colleagues put together a collection of packages nested inside of the package `tidyverse`.

```{r preliminaries}
library(tidyverse)

```

Each package is focused on different tasks:

- `ggplot2`: create elegant data visualizations using the grammar of graphics
- `purrr`: functional programming tools
- `tibble`: updated `data.frame()`s for modern quality of life improvements
- `dplyr`: data manipulation
- `tidyr`: tidy messy data (steps 1-3)
- `stringr`: common string manipulations and operations
- `readr`: user-friendly reading of standard data formats (`.csv`, `.txt`, `.fwf`)
- `forcats`: tools for working with categorical variables (factors)

To learn more about the functions available inside of each of these packages such as `dplyr` (or any other package), use the command `help(package = "dplyr")`.

## Cleaning Current Population Survey Data
************************************************************

The standard way to load in data is to use R's base function `read.csv()` with the `file` argument set equal to the complete path to the file on your computer:

```{r read.csv, eval = FALSE}
cps = read.csv(file = 'C:/Users/johnj/Dropbox/Julian/Academia/UIUC/Semester 11 Fall 2022/ECON474/Data/CPS Dealing with Data.csv')
```


But take a look at this:
```{r ?read.csv, eval = FALSE}
?read.csv
```

The argument `file` is the first argument of `read.csv()`, so we don't need to specify it:
```{r read.csv2, eval = FALSE}
cps = read.csv('C:/Users/johnj/Dropbox/Julian/Academia/UIUC/Semester 11 Fall 2022/ECON474/Data/CPS Dealing with Data.csv')

```
But as I am sure you have noticed, that took a long time to load the data. 
Fortunately, the `tidyverse` contains the package `readr`, which has the function `read_csv()` amongst others.
The package is focused around providing updated and faster read functions with a few bells and whistles, all integrated to be part of the `tidyverse`.

Now, we could just use the code chunk above and replace the functions, but we are going to do a bit of planning ahead.
In this demonstration Dealing with Data, we are also going to read data directly from a website via a line of code. 
It is an important practice to  **always minimize website data requests** because it prevents unecessary strain on the hosts servers. 
An excessive amount of requests can crash their servers and may result in you being banned.
Provided the data is a service you are accessing for free.

Now that web ethics are addressed, we are therefore going to use two steps to load in data:

1. create an object that is the path to the folder on your computer that contains the data
2. *paste* together the folder path with the data file name to load in the data

We will come back to the folder path object later.

```{r path}
path_data = 'C:/Users/johnj/Dropbox/Julian/Academia/UIUC/Semester 11 Fall 2022/ECON474/Data/'
file_cps = paste0(path_data, 'CPS Dealing with Data.csv')
print(file_cps)

cps = read_csv(file_cps)

head(cps)
```
We can see a few things from printing the the `head()` of the data.

1. We have 19 variables
2. At least one has missing values
3. All variables have a numerical coding
4. Some variables appear to have a special coding (i.e. `OCC2010 = 9999` or `EARNWEEK = 9999.99`)

Looks like we are going to have to do some cleaning!

Let's take a closer look at one variable to make a point: `STATEFIP`.
This variable contains codes for states using the Federal Information Processing Standards (FIPS).
We will print out the unique values by selecting the column with a `$` by `cps$STATEFIP` as the first argument of the `unique()` function.
We can then **tab**ulate the number of observation per value using the `table()` function.

```{r state fips}
unique(cps$STATEFIP)
table(cps$STATEFIP)
```
**CAUTION**: we need to be particularly careful with _**numerically coded**_ variables in a regression. 
Illinois FIPS code is 17, Colorado is 8, and Alabama is 1.
Naively plugging in these variables tells R that 

$$Illinois = 2\times Colorado + Alabama$$

or 

$$Illinois = 10 \times Colorado - 43 \times Alabama$$
While I am sure local s can figure out jokes that satisfy these conditions, this is not what we want when running regressions because states are fundamentally *categorical variables*.
Therefore, we instead need to clean this variable and the other categorical variables like it so we can have regressions with separate variables that indicate whether or not the observation belongs to each state.


Neither you nor I want to manually type out the 50 states (and DC!).
We will come back to this later when we are dealing with the COVID-19 data, because it has state FIPS codes and state names.
While we will be merging in the COVID data, we will also be using it as a **crosswalk**.


For now, we are going to clean a few different variables:

- [`MARST`](https://cps.ipums.org/cps-action/variables/MARST#codes_section)
- [`SEX`](https://cps.ipums.org/cps-action/variables/SEX#codes_section)
- [`RACE`](https://cps.ipums.org/cps-action/variables/RACE#codes_section)
- [`EMPSTAT`](https://cps.ipums.org/cps-action/variables/EMPSTAT#codes_section)
- [`EDUC`](https://cps.ipums.org/cps-action/variables/EDUC#codes_section)
- [`EARNWEEK`](https://cps.ipums.org/cps-action/variables/EARNWEEK#codes_section)


We are going to use the `mutate()` function to *add new variables*. 
For example:
```{r month2}
cps = cps %>%
  mutate(month2 = MONTH^2)

```
Okay, what we just did does not make any sense.
But, R is happy to add a new column on to our data frame.
To show that the variable has been added, we can look at the variable names with the function `names()`.

```{r motnh2 check}
names(cps)
```
And there it is.

To clean the data, (i.e. dealing with bad `NA` codes), we are going to use the `mutate()` to *overwrite existing variables*.
We are overwriting the variables because there is no reason (that I can think of) to keep the messy variables.
Our cleaning is going to use three different methods **inside** of the mutate function:

1. a logic statement - logic statements are useful if you wish to create **dummy variables**. 
Dummy variables indicate whether or not some other variable is true.
Logic statements return either `TRUE` or `FALSE`, however, we are going to coerce these values to numbers by multiplying them by 1.
Here, we are going to clean the married variable (`marst`) to take the values:
    - 1: the individual **is** married
    - 0: the individual **is not** married
2. `ifelse(arg1, arg2, arg3)` - this function is just like a logic statement, but it has three arguments:
    1. `arg1`: a logic statment
    2. `arg2`: what to code the variable as if the logic statment is `TRUE`
    3. `arg3`: what to code the variable as if the logic statement is `FALSE`. 
    Note that it is possible to create a categorical variable using multiple conditions by setting `arg3 = ifelse(...)`.
    This is called a *nested* `ifelse()` statement.
3. `case_when(arg1, arg2, ...)` - this funtion is from the package `dplyr`, which in my opinion is a much cleaner way to use nested logic statements to create categorical variables.. 
    - Each of the arguments need to be `fromula` objects (`y ~ x`), where the left hand side is a logic statement (`age >= 62 ~ x`) and the right hand side is what to encode the variable as (`age >= 62 ~ "can retire"`).
    - You nest logic statements by chaining multiple formulas: `case_when(age >= 62 ~ "can retire", age < 18, "cannot vote")`.
    - So what are those who are between ages 18 and 61 coded as? By default, there are coded as `NA`s, which may be desireable. 
    In the following example, we are going to show two ways to complete the labelling.

 
 

```{r cleaning cps}
names(cps) = tolower(names(cps))

cps = 
  cps %>%
  mutate(married = (marst < 3)*1,
         sex = ifelse(sex == 1, 'male', 'female'), # is female = 2x male? well maybe
         empstat = ifelse(empstat %in% c(10 ,12),
                          'employed',
                          ifelse(empstat %in% c(21, 22),
                                 'unemployed',
                                 'NILF')),
         race = case_when(race == 100 ~ 'white',
                          race == 200 ~ 'black',
                          race == 300 ~ 'indigenous',
                          race %in% c(651, 652) ~ 'asian',
                          race > 652 ~ 'multi'),
         degree = case_when(educ == 125 ~ 'phd',
                            educ == 124 ~ 'professional',
                            educ == 123 ~ "master's",
                            educ == 111 ~ "bachelor's",
                            educ >= 91 ~ "associate's",
                            educ >= 73 ~ 'high school',
                            educ >= 2 ~ 'none'),
         earnweek = case_when(earnweek < 9999.99 ~ earnweek))
```

There are still several variables that are not full cleaned, for example `occ2010` or `uhrsworkt`. 
We will have to be bare this in mind when moving forward.

### `datetime` objects
****************************************

The CPS is a monthly survey, where each individual participates over 16 months.
The survey follows a 4-8-4 format, where individuals are survey for 4 months, are not surveyed for the next 8 months, and then are survey again for 4 more months.


Every March, participants are asked detailed questions about their income in the Annual Social and Economic Supplement (ASEC).
But I have specifically omitted this supplement from these data because we are focusing on unemployment statistics.
The omitted supplement is why we have the column of `NA`s `ASECFLAG`.
Wage data are also collected for each out-going rotation group (month 16).
We will use this portion of the data later in the LAB.

You typically do not need to deal with date objects with annual data because you simply use the year as an integer.
Treating periods of times as integers becomes problematic when your data have a more granular definition (i.e. monthly, daily, hourly, etc.) because months (or years) do not have the same amount of days.
Fortunately, `R` is clever and can account for this.

While there are functions available inside of base `R`, I much rather prefer to use the package `lubridate` because it SO much more user friendly.
Since we are intending to plot monthly unemployment rates, we are going to create a `datetime` variable called `date` by combining the variables `year` and `month` from the CPS data in the function `ymd()` from `lubrdiate`.


```{r cps date}
# install.packages('lubridate')
library(lubridate)

# names(cps)
# cps %>% select(month, year)
# paste0(cps$year, '-', cps$month)
# ym(paste0(cps$year, '-', cps$month))
# ymd(paste0(cps$year, '-', cps$month, '-1'))

cps = cps %>%
  mutate(date = ym(paste0(cps$year, '-', cps$month)))
class(cps$date)
```

## Unemployment Rates 
************************************************************

Recall that unemployment rates are calculated as:

$$U\text{-}rate = \frac{Unemployed}{Labor\text{ }Force}\times 100 \% = \frac{U}{U + E}\times 100 \%$$


It is important to note that the data we are using come from a survey. 
This matters because the sampled respondents may not be a representative sample to the population. 
Consequently, we need to weight each observation by the inverse probability of that observation being sampled to estimate aggregate statistics such as the unemployment rate.

The inverse probability weighting $w_i = \frac{1}{prob[i]}$ makes individuals more likely to be sampled (i.e. demographic majorities) receive less weight than those who are less likely to be sampled (i.e. demographic minorities).
The math is a bit involved to show this, but doing this process provides an unbiased estimate (due to sampling) of the true population.

Our goal is to produce the 

- unemployment rate 

for each

-  month.

That means we need to calculate $U_t = \sum_{i=unemployed_t} w_{i,t} \times 1$ and $E_t = \sum_{i=employed_t} w_{i,t} \times 1$.
Observe that both equations have a "$\times 1$" for individual $i$ at month $t$. 
This is because we can actually calculate the total number of unemployed as $\sum_i w_{i,t} \mathbb{I}\{i = unemployed\}$, where $\mathbb{I}\{i = unemployed\}$ equals $1$ if an individual is unemployed and $0$ otherwise.
In other words: **a logic statement**.
The same concept applies to the employed.

Now we need to tell R how to perform these calculations.
Specifically, we need to *summarize* the individual-level data to aggregated unemployment rates for each month for the entire country each month.
That sounds like manipulating data to me which means:

$$\huge{\text{dplyr!}}$$

### Chaining Functions with `%>%`
****************************************

The first step is to tell `R` to grab the CPS and data and `group_by(date)`.
Once the data are grouped over each time period, we can then create  the variables $U$ and $E$ to `summarise()` the individual-level data to calculate the unemployment rates.
Don't forget those population rates!

```{r urate}
cps = group_by(cps, date)
urate = summarise(cps,
          U = sum((empstat == 'unemployed')*wtfinl),
          E = sum((empstat == 'employed')*wtfinl),
          urate = U/(U + E)*100)
urate
```

Check this out: we can go back to the [official unemployment numbers](https://fred.stlouisfed.org/series/UNRATE) for 2020 and see that our numbers match! How cool!?

As a technical note, what we just did required two lines of code (and one to print).
We can actually do it one line using the `%>%` function to *chain* functions.
The `%>%` function puts whatever *precedes* it (i.e. a data frame) as the first argument to the *following* function


```{r urate oneline}
cps %>%
  group_by(date) %>%
  summarise(U = sum((empstat == 'unemployed')*wtfinl),
          E = sum((empstat == 'employed')*wtfinl),
          urate = U/(U + E)*100)
```

### `select()`ing variables and `filter()`ing observations
****************************************

This class is focused towards students earning a bachelor's degree in economics.
We can determine the unemployment rate for this subset of individuals using `filter()` to only "let in" observations with economist occupations and a bachelor's degree.
We will use the [`OCC2010`](https://cps.ipums.org/cps-action/variables/OCC2010#codes_section) variable code of 1800 for "Economists and Market Researchers."
I don't need the other variables, so I am going to `select()` the variables `date` and `urate`.

```{r urate econ}

cps %>%
  filter(occ2010 == 1800 &
           degree == "bachelor's") %>%
  group_by(date) %>%
  summarise(U = sum((empstat == 'unemployed')*wtfinl),
          E = sum((empstat == 'employed')*wtfinl),
          urate = U/(U + E)*100) %>%
  select(date, urate)

```
Well that seems like a good job to get!
Okay, but what is really going on?


```{r urate econ2}

cps %>%
  filter(occ2010 == 1800 &
           degree == "bachelor's") %>%
  group_by(date) %>%
  summarise(U = sum((empstat == 'unemployed')*wtfinl),
          E = sum((empstat == 'employed')*wtfinl),
          urate = U/(U + E)*100,
          n = n())

```
It appears that trimming the data to those with a bachelor's degree doing economics or market research gives us small sample bias.

### Arranging Data
****************************************

We currently have the data organized sequentially by `date`.
If we instead want to organize the variable by `urate`, we can do so with the function `arrange()` from `dplyr`.

```{r arrange}
urate %>%
  arrange(urate)
```
While it is interesting to know that February had the lowest unemployment rate, I want to know the ordering of the months from highest unemployment rate to lowest.
This will require wrapping the variable `urate` with the `desc()` function inside of the `arrange()` function.


```{r arrange desc}
urate %>%
  arrange(desc(urate))
```

Data can also arrange by multiple variables 
Suppose we are interested in comparing the unemployment rates to GDP.
Since GDP is a quarterly series, we need to adjust the CPS data to be quarterly.
In comes the function `quarter()` from the `lubridate` package. 

```{r group arrange}
urate %>%
  mutate(q = quarter(date)) %>%
  arrange(q, desc(urate))

```




# Exploratory Data Analysis
********************************************************************************

I don't know about you, but I am having a hard time understanding the patterns in the data that are printed out in those tables.
I also don't have any checks to know whether or not I have properly cleaned the data.
I would really like to plot the data we just made too.
Fortunately, Dr. Wickham and colleagues thought of this by incorporating `ggplot2`, an easy-to-use plotting package with many options for customization.

`ggplot2` has many different types of plots, most of which follow the naming convention `geom_xxxx()`. 
It has many interesting non-standard plots such as violin plots or hex plots to help you or your audience better understand your data.
[R4DS](https://r4ds.had.co.nz/exploratory-data-analysis.html) has some good examples.
There are many packages further the customization.
For example`ggthemes` has built in themes to match popular themes such as the Economist magazine and other visual modifications
The package `latex2exp` has a function `TeX()` that comes in handy if you need to include $\LaTeX$ in your figure labels. 
The `cowplot` package puts figures into a grid using its `plot_grid()` function.
A quick web search provides many examples.




## National Unemployment Rate
**********************************************************************

We spent so much effort creating those unemployment rate numbers, we might as well take a look at them.

Creating a figure in `ggplot2` behaves similarly to chaining functions.
Instead of using the `%>%` operator, we use `+`.
Every figure from `ggplot2` starts the same way with the `ggplot()` initialization function.
The first argument is the name of you data (i.e. `urate`), and the second argument is a mapping of data variables (`date`, `urate`) to figure variables (`x`, `y`) declared in the `aes()` (aesthetic) function.
However, it is also possible to instead specify the data and `aes()` inside of the various `geom_xxxx()` functions.

Anyway, let's create a line plot with points.


```{r ggplot national}
ggplot(urate, aes(x = date, y = urate)) +
  geom_point() +
  geom_line() +
  labs(x = 'Month',
       y = 'Unemployment Rate (%)',
       title = '2020 Unemployment Rate',
       subtitle = 'Source: CPS') +
  theme_minimal()
```

That is the maximum effort I am going to put into making a figure look pretty.
What follows is the going to be focused on formatting the data for plotting instead of the plotting itself.


## Education-based Unemployment Rates
**********************************************************************

I am curious if there are any discrepancies in unemployment rate for different subsets of the data.
Let's use the example of level of education (`degree`).
I want a line on my figure for each `degree` present in the data.


```{r urate degree}
urate_degree = cps %>%
  group_by(date, degree) %>%
  summarise(
          U = sum((empstat == 'unemployed')*wtfinl),
          E = sum((empstat == 'employed')*wtfinl),
          urate = U/(U + E)*100) %>%
  na.omit()

ggplot(urate_degree, aes(x = date, y = urate, color = degree)) +
  geom_line()
```

But what if we want to compare to the national average?

```{r urate degree national}
ggplot() +
  geom_line(data = urate_degree, aes(x = date, y = urate, color = degree)) +
  geom_line(data = urate, aes(x = date, y = urate, color = group), color = 'black', linetype = 'dashed') 
```

The national average line is not in the legend! 
One way is to force it by stacking `urate` on top of `urate_degree` using `bind_rows()` to maintain the three requirements for "tidy" data.
`bind_rows()` requires the data frames to have the same variables, but `urate_degree` has the additional variable `degree`.
We can work around this by creating a variable in `urate` called `degree`. 
To help us with the figure, we can label the value to be `"national avg."`.
I also want the `linetypes` to be different, so I will create a dummy variable indicating whether or not the line is an average.

```{r urate degree national2}
urate$degree = 'national avg.'

urate_nat_degree = bind_rows(urate, urate_degree) %>%
  mutate(avg = degree == 'national avg.')

ggplot(urate_nat_degree, aes(x = date, y = urate, color = degree, linetype = avg)) +
  geom_line()
```


# Multiple Data
********************************************************************************

It is incredibly common to combine multiple data sets in an analysis.
For example, we are going to examine the relationship of COVID-19 cases with state-level unemployment rates.
We are also going to explore how average weekly wages were impacted, which requires loading in Consumer Price Index data to obtain *real* wages.
[R4DS](https://r4ds.had.co.nz/tidy-data.html) boils the "tidy" data concept down further in the core components:

1. Put each data set in a tibble
2. Put each variable in a column


## COVID-19
**********************************************************************

We are going to directly pull the data from Github using the base `R` function `download.file()`.
Note that the code chunk is set to `eval = FALSE` so we do not run it everytime we knit this document.

```{r covid download, eval = FALSE}
download.file('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv',
              paste0(path_data, 'covid19.csv'))

```

Now that the data are safely squared away, we can load in the COVID data and look at the first few rows.

```{r covid}
covid = read_csv(paste0(path_data, 'covid19.csv'))
head(covid)
```

These data come with 11 keys! 
And importantly, the data are formatted wide: the date variable is the column names.
Before we can ensure each variable is it's own column, we need to clean a litte.
Collectively we will:

1. `filter()` to have `code3 == 840` to ensure we only have states (sorry Guam)
2. use a negative `select()` to remove `UID`, `iso2`, `iso3`, `code3`, `Admin2`, `Country_Region`, `Lat`, `Long_`, and `Combined_Key`
3. overwrite the `FIPS` variable to change from county codes to state by `floor()` dividing by 1000
4. `group_by()` the data on `FIPS` and `Province_State` to
5. `summarise(across(everything(), fn))` to aggregate the data from county-level to state-level
6. `filter()` out FIPS codes greater than 56 (Wyoming) to remove "other" state locations
7. `pivot_longer()` by selecting the dates with a `:`
8. `mutate()` the `date` variable to a `datetime` object and then `floor_date()` `date` to months
9. `group_by()` the three keys to
10. create the summary static of the sum for each month-state


```{r}
covid_state = covid %>%
  filter(code3 == 840) %>%
  select(-UID, -iso2, -iso3, -code3, -Admin2, -Country_Region, -Lat, -Long_, -Combined_Key) %>%
  mutate(FIPS = floor(FIPS/1000)) %>%
  group_by(FIPS, Province_State) %>%
  summarise(across(everything(), mean)) %>%
  filter(FIPS <= 56) %>%
  pivot_longer(cols = `1/22/20`:names(covid)[ncol(covid)],
               names_to = 'date',
               values_to = 'cases') %>%
  mutate(date = mdy(date),
         date = floor_date(date, unit = 'month')) %>%
  group_by(FIPS, Province_State, date) %>%
  summarise(cases = sum(cases))
```

Excellent! Now we just need to use the CPS data to create state-date-level unemployment rates and create a variable `pop` to combine with the COVID data for cases per capita.

```{r}
urate_state = cps %>%
  group_by(statefip, date) %>%
  summarise(urate = sum(wtfinl*(empstat == 'unemployed')) / (sum(wtfinl*(empstat == 'unemployed')) + sum(wtfinl*(empstat == 'employed'))),
            pop = sum(wtfinl))

```


## Keys, Values, and Joins
**********************************************************************



The following images are from [R4DS](https://r4ds.had.co.nz/relational-data.html)


Related data sets have a variable, or **key**, that links them together. 
For example, we are going to like the unemployment data and the COVID cases data together by state FIPS codes or `statefip`.
It is entirely possible to have *more than one key.*
Again, we are actually linking the data together by `statefips` and `date`.
While keys are essential for combining data together, what we are really interested in are the **values** contained within each data set.
In this case, the value of the CPS data is `urate` and the value from the COVID data are the number of cases. 


Suppose we have the following two data sets `x` and `y` where the colored numerical columns are the keys.

![](join-setup.png)

The conceptual framework is to rotate the data frames by 45 degrees to overlap the keys.

![](join-setup2.png)


The simplest join is the `inner_join()`. 
Only the observations with matching keys in both data sets are kept.

![](join-inner.png)

This then brings us to the other cases of `left_join()`, `right_join()`, and `full_join()`.
Depending on the type of join, keys only present in one data set that are not present in the other *may* be filled with NAs.

![](join-outer.png)


One must also pay particularly close attention if there are key duplicates.
This can lead to the number of observations **exploding** beyond the memory limits of your computer.



![](join-one-to-many.png)

![](join-many-to-many.png)

The various `xxxx_join()` functions automatically look for keys by finding variable name overlap.
If they do not find any variables with overlap, it does not know what to do.
Our situation has two keys: the state and date.
Because the state keys have different names, we need to specify *BOTH* of the key matches inside the join function.

Here we will use the `inner_join()` function.
We mustn't forget to also calculate the cases per capita!


```{r the join}
df_state = inner_join(urate_state, covid_state,
          by = c('statefip' = 'FIPS',
                 'date' = 'date')) %>%
  mutate(cases_per_cap = cases/pop)

df_state %>% head()

```

Aaaand that's it. 
We are ready to explore the relationships in these merged data. 


# Lab
********************************************************************************

No it's your turn! 
You get to practice everything we did above and apply a few *new* functions with guidance.
We are going to compare `cases_per_cap` versus real weekly earnings and examine the distribution of state-level unemployment by creating two figures.
Obtaining real weekly wages requires a consumer price index (CPI). 
We are going to use the urban CPI (CPI-U) from the [Bureau of Labor Statistics (BLS)](https://www.bls.gov/cpi/data.htm).

## Tidying CPI Data
**********************************************************************

What joy! 
The data from the BLS are formatted in a rather annoying way.
The data are also in an Excel file, which is not handled in the `readr` package.
As you will see, truly every messy dataset is messy in its own way...

Steps:

1. Open the excel file `CPI-U.xlsx` to see the fun ahead of us.
2. install the `readxl` package
3. attach the package to our working session with `library()`
4. use `?read_excel` to look at the arguments of `read_excel()`
5. create an object named `cpi` using `read_excel()`
    i. set the first argument, `path`, to the full path to `CPI-U.xlsx`. *Hint: don't forget* `path_data`.
    ii. set the `range` argument to grab the table from the cell that has the variable name `Year` to the cell that contains the CPI for December 2022. Take a look at the documentation (`?read_excel`) if you are uncertain how to specify this argument.
6. print the head of `cpi`


```{r read cpi}
library(readxl)

cpi = read_excel(paste0(path_data, 'CPI-U.xlsx'),
                 range = 'A12:M23')
head(cpi)
```

The CPI-U data are standardized to have the average of 1982-1984 to be the base period.
That was nearly three decades ago and doesn't mean much to me.
We are going to adjust the base year to 2020 by creating a object that is equal to the average CPI-U of 2020 in one line by chaining several functions from `dplyr`.

Steps:

1. Using `cpi`, create an object named `cpi_2020` by
2. `filer()` to only have data from the `Year` 2020
3. omit the variable `Year` by performing a negative `select()`ion
4. chain the function `rowwise()` without any specified arguments
5. `summarise()` the average data of the year by using the function `mean()` over year by inputting the first argument as `Jan`:`Dec`
6. `pull()` out the summarised value


```{r cpi 2020}
cpi_2020 = cpi %>%
  filter(Year == 2020) %>%
  select(-Year) %>%
  rowwise() %>%
  summarise(m = mean(`Jan`:`Dec`)) %>%
  pull()
```

The data provided by the BLS is not in a tidy format: the variable names are variable themselves.
This is going to require some help from the `tidyr` package!

Steps:

1. using `cpi`, overwrite the object `cpi` by:
2. `pivot_longer()` the data 
    i. since the data only have key (`Year`), we can easily pivot longer by setting the first argument to `-Year`
    ii. set `names_to` equal to `month`
    iii. set `values_to` equal to `cpi_u`
4. `mutate()` the variable
    i. `date` by using the `lubridate` function `ym()` by pasting together the variables `Year` and `Month` seperated with `'-'`
    ii. `cpi` by dividing `cpi_u` divided by `cpi_2020` to create a new base and then multiply it by 100
5. `select()` the variables `date` and `cpi`

```{r cpi tidy}
cpi = cpi %>%
  pivot_longer(-Year,
               names_to = 'month',
               values_to = 'cpi_u') %>%
  mutate(date = ym(paste0(Year, '-', month)),
         cpi = cpi_u/cpi_2020*100) %>%
  select(date, cpi)
```


## Joins
**********************************************************************

Schweet! 
Now we get to create the inflation-adjusted real weekly earnings.
Specifically, we are going to do this for each state to compare with the variables inside of `df_state`.

Steps:

1. using `cps`, a new object named `df` and
2. `filter()` such that all observations have weekly earnings without `NA` cells using `is.na()`. *Hint:* `!FALSE = TRUE`.
3. `inner_join()` `cps` with `cpi` on they key the corresponds to the date. Note that the key has the name in both data frames.
4. `mutate()` the variable `earnweek_r` by multiplying the variable `earnweek` by `cpi` and dividing by 100
5. `group_by()` the data on `statefip` and `date`
6. `summarise()` a new variable named `m_r_wk_earn` using the function `weighted.mean()` by setting the first argument `x` equal to `earnweek_r` and the second argument `w` equal to `wtfinl`
7. `inner_join()` with `df_state` on they key the corresponds to the date. Note that the key has the name in both data frames.

Then print out the head of `df`.

```{r join cps cpi}
df = cps %>%
  filter(!is.na(earnweek)) %>%
  inner_join(cpi) %>%
  mutate(earnweek_r = earnweek * cpi/100) %>%
  group_by(statefip, date) %>%
  summarise(m_r_wk_earn = weighted.mean(earnweek_r, wtfinl)) %>%
  inner_join(df_state)

head(df)
```



## EDA
**********************************************************************

Using `ggplot2`, produce a scatter plot (`geom_point()`)

- of mean real weekly earnings on COVID-19 cases per capita
- using only monthly data from Illinois


```{r scatter}
ggplot(filter(df, Province_State == "Illinois"),
              aes(x = cases_per_cap, y = m_r_wk_earn)) +
  geom_point()

```

Now we get to make one of my favorite plots: the violin plot.
We are going to look at the distribution 
It will be super squished and have few observations per category, but I love them so much we are going to do it anyway.

Steps:

1. set the `data` argument of `ggplot()` to `df` and set the `mapping` arugment equal to `aes()` where `x = Province_state` and `y = urate`
2. chain on `geom_violin()`
3. then flip the coordinates for the sake of having easier to read labels using `coord_flip()`


```{r violin}
ggplot(df, aes(x = Province_State, y = urate)) +
  geom_violin() +
  coord_flip()
```


